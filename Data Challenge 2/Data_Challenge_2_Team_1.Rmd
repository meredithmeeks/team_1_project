---
title: "QMSS5069 Data Challenge 2"
author: "Team 1 - Svyatoslav Andriyishen, Shilpa Sure, Meredith Meeks, Kai Huo, Hannah Hao"
date: "March 22, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Challenge 2

1. Ask two (2) questions that might help you understand better the dynamics of violence contained on our datas et. Apply one algorithm per question and share your insights from each analysis. [50 pts]  Remember: a non-finding is also a finding! It tells you whether a question is worth pursuing further or not.
	* perform the necessary transformations in your data - if any are needed, and explain why you did that
	* show the output from your analysis in a consumable form
	* be explicit about the limitations of your anaylisis, due to estimation or to the data itself
	* did you find something interesting? what is that? does your finding suggest this question is worth pursuing further? why or why not?
	* if you did not find something interesting, explain why, and whether there is some additional information that would help in answering your question
	* provide your code, and a single visualization per question that summarizes your finding 
	* phrase your finding for each question in two ways: 
		* one sentence that summarizes your insight
		* one paragraph that reflects all nuance in your insight 
	* make sure to also include your code 

2. Formulate two (2) conditional hypotheses that you seek to investigate with the data. One of your hypotheses should condition on two variables (as the example on the slides), and the other should condition on three variables. [50 pts]
    * formulate each one of your hypotheses explicitly in substantive terms (as opposed to statistical terms) using 2-3 lines at most
	* show exactly how each one of your hypotheses translates into the marginal effect that you will seek to estimate from the data 
	* show the output from your analysis in a consumable form
	* show all your computations to estimate the corresponding marginal effect and its standard error
	* be explicit in your assumptions
	* be explicit in the limitations of your inferences
	* phrase your finding for each question in two ways: 
		* one sentence that summarizes your insight
		* one paragraph that reflects all nuance in your insight 
	* make sure to also include your code 

This is a team assignment. Please create a file on your team GitHub repo where you answer the challenge, including links to your code, and graphs.

```{r Libraries}
if (!require("dplyr")) {
 install.packages("dplyr", dependencies = TRUE)
 library(dplyr)
 }
if (!require("tidyr")) {
 install.packages("tidyr", dependencies = TRUE)
 library(tidyr)
 }
if (!require("ggplot2")) {
 install.packages("ggplot2", dependencies = TRUE)
 library(ggplot2)
 }
if (!require("readr")) {
 install.packages("readr", dependencies = TRUE)
 library(readr)
}
if (!require("pscl")) {
 install.packages("pscl", dependencies = TRUE)
 library(pscl)
 }
if (!require("MASS")) {
 install.packages("MASS", dependencies = TRUE)
 library(MASS)
 }
if (!require("boot")) {
 install.packages("boot", dependencies = TRUE)
 library(boot)
 }
if (!require("AICcmodavg")) {
 install.packages("AICcmodavg", dependencies = TRUE)
 library(AICcmodavg)
}
if (!require("lubridate")) {
 install.packages("lubridate", dependencies = TRUE)
 library(lubridate)
}
if (!require("randomForest")) {
 install.packages("randomForest", dependencies = TRUE)
 library(randomForest)
}
```

```{r Load Data}
Confrontations <- read.csv("AllViolenceData_170216.csv")
Confrontations$date <- as.Date(Confrontations$date)
# take out global.id, event.id, unix.timestamp
Confrontations <- Confrontations[ , -c(which(colnames(Confrontations) == "global.id"),
                                       which(colnames(Confrontations) == "event.id"),
                                       which(colnames(Confrontations) == "unix.timestamp"))]
str(Confrontations)
```

#### Question 1

##### Part 1

We are looking for the effect of independent variables on the number of non-law enforcement persons (civilians and organized crime members) wounded.

```{r}
civ_oc_wounded <- Confrontations$organized.crime.wounded + Confrontations$civilian.wounded
Confrontations_2 <- cbind(Confrontations, civ_oc_wounded)

ggplot(Confrontations_2, aes(civ_oc_wounded)) + 
  geom_bar(fill = "white", colour = "darkgreen") + theme_bw() + 
  xlab("Civilians and Organized Crime Wounded") + ylab("Count") + 
  facet_wrap(~ perfect.lethality)

# mean
mean(Confrontations_2$civ_oc_wounded)

# variance
var(Confrontations_2$civ_oc_wounded)
```

We notice two things about the data - one, there are a large number of cases in which zero civilians or organized crime members were wounded. Second, the variance of the distribution (1.55) is much larger than the mean (0.44).

In cases in which the variable is a count, there is a large number of zeros, and the variance is much larger than the mean, it may be better to use a zero-inflated negative binomial regression, rather than a linear regression model. 

For comparison, let us first look at the performance of a linear regression model. We have created one independent variables to reduce collinearity - arms_seized which includes the number of long guns seized and small arms seized. We are also including variables for the presence of the army, navy, afi, federal police, municipal police, ministerial police, and state police. 

Here is the linear model:

```{r}
#independent variables:
Confrontations_2$arms_seized <- Confrontations_2$long.guns.seized + Confrontations_2$small.arms.seized
#Confrontations_2$cart_clips_seized <- Confrontations_2$cartridge.sezied + Confrontations_2$clips.seized
ols_civ_oc_wounded <- glm(civ_oc_wounded ~ arms_seized + vehicles.seized + army + navy + afi + federal.police + municipal.police + ministerial.police + state.police, Confrontations_2, family="gaussian")
summary(ols_civ_oc_wounded)
```

The linear model fit is quite poor - as we saw with the distribution of the dependent variable, there is a high number of zero cases. Now we can look at the same model, but modeled as a zero-inflated negative binomial distribution:

```{r}
znb_civ_oc_wounded <- zeroinfl(civ_oc_wounded ~ arms_seized + vehicles.seized + army + navy + afi + federal.police + municipal.police + ministerial.police + state.police, data = Confrontations_2, dist = "negbin", EM = TRUE)
summary(znb_civ_oc_wounded)
AICc(znb_civ_oc_wounded, return.K = FALSE, second.ord = FALSE)
```

Comparing the AIC values, we can see that the zero-inflated negative binomial model performs much better the linear model. Additionally, since the zero-inflated negative binomial model assumes that zeros are generated by two processes - situations where the only possible outcome is zero, and situations where the count is zero. In the model output, the "Count model coefficients" gives us negative binomial regression coefficients, while the "Zero-inflation model coefficients" output gives is coefficients for predicting excess zeros. 

While the model fit is better with a zero-inflated negative binomial model, limitations arise in the interpretability of the model compared to OLS. The coefficients for our negative binomial model (the "Count model coefficients") are given in log-counts. For example, looking at the coefficient on "army", we can interpret this as saying that confrontations involving the army have a 0.516 lower expected log-count of civilians and organized crime members wounded, holding all other factors constant. Looking at the zero-inflated model coefficients, specifically at the coefficient on "federal.police", the log odds of being an "excessive zero" increase by 2.54 when the federal police are involved - when the federal police are involved, there is a higher likelihood of no wounded civilians or organized crime members. 

For our marginal effects, we are primarily interested in looking at the count model coefficients. In this model, the following variables are siginificant with a p-value less than 0.001: arms_seized, vehicle.seized, army, municipal.police, ministerial.police, and state.police. 

Since log-counts are not very intuitive to understand, we can exponentiate the coefficients to get rid of the log:  

```{r}
exp(coef(znb_civ_oc_wounded))
```

When we exponentiate coefficients, we can understand them as incident rate ratios. Looking at the count model coefficients for our significant predictors, we can interpret them as follows:

- **arms_seized**: Confrontations in which long guns or small arms are seized have an incident rate ratio of civilians and organized crime members wounded that is 1.05 times higher than confrontations where these are not seized, holding all other variables constant. 
- **vehicles.seized**: Confrontations in which vehicles are seized have an incident rate ratio of civilians and organized crime members wounded that is 1.11 times higher than confrontations where these are not seized, holding all other variables constant. . 
- **army**: Confrontations in which the army is involved have an incident rate ratio of civilians and organized crime members wounded that is 0.59 times that of confrontations in which the army is not involved, holding all other variables constant. .
- **municipal.police**: Confrontations in which the municipal police are involved have an incident rate ratio of civilians and organized crime members wounded that is 0.44 times that of confrontations in which the municipal police are not involved, holding all other variables constant. 
- **ministerial.police**: Confrontations in which the ministerial police are involved have an incident rate ratio of civilians and organized crime members wounded that is 0.47 times that of confrontations in which the ministerial police are not involved, holding all other variables constant. 
- **state.police**: Confrontations in which the state police are involved have an incident rate ratio of civilians and organized crime members wounded that is 0.67 times that of confrontations in which the state police are not involved, holding all other variables constant. 

To summarize the main finding from this algorithm:

From a zero-inflated negative binomial model of the number of civilians and organized crime members wounded, we can see that incident rate ratios of this occuring are higher when arms (long guns or small arms) or vehicles are seized, and lower when the army, municipal police, ministerial police, or state police are involved. 

The nuance with this type of model comes with the balance between model fit and interpretability. While the zero-inflated negative binomial distribution fits the model better than the linear model, the coefficients are not as easily interpretable as in a linear model. In our model, the direct coefficients are log counts, and exponentiated we can consider them to be incident rate ratios. This tells us about the relative impact of each variable as a ratio of incidence rates, but this is not as easily interpretable as an OLS coefficient. The finding that we have found, that the rates of civilians and organized crime members wounded is higher in situations where arms or vehicles are seized, is quite interesting and may tell us more about the types of confrontations in which civilians or organized crime members are wounded. We also see that incident rate ratios of woundings are lower with certain types of law enforcement (army, municipal police, ministerial police, state police). Both of these findings are quite interesting and should be pursued further - we can ask questions like do those law enforcement bodies use different policies than others? Or is it a matter of which law enforcement parties are generally involved when arms or vehicles are seized?

##### Part 2

We are using randomForest package to build an algorithm predicing the number of civilians dead in a given confrontaion.

```{r}
# randomForest algorithm

set.seed(34593475)
# may have to use rfImpute to impute missing values but dropping for now

Confrontations_nafix <- Confrontations[ , apply(Confrontations, 2, function(x) !any(is.na(x)))]

rf <- randomForest(x = Confrontations_nafix[ , -which(colnames(Confrontations_nafix) == "civilian.dead")], 
                   y = Confrontations_nafix[ , which(colnames(Confrontations_nafix) == "civilian.dead")], 
                   ntree = 100, importance = TRUE, proximity = TRUE)

rf

plot(rf$mse)

varImpPlot(rf, main = "Variance Importance Plot",
           cex = .6, pt.cex = .8, pch = 6)

# class(rf$importance[,1])

#ggplot(as.data.frame(rf$importance), 
#       aes(x = rf$importance[,1], y = reorder(rownames(rf$importance), rf$importance[,1]))) +
#  geom_point() +
#  theme(axis.text.y = element_text(size = 6))

```

The most important factor in predicting the number of civilians dead in a confrontation is the number of municipal police dead. The second and third most important factors are the total number of deaths in a confrontation and the factor variable of municipal police involvement. Although randomForest results are not as interpretable as OLM or ligistic regressions, the high importance of these factors in combination with low MSE and a very large portion of the variance explained can be insightful. Perhaps incidents involving the municipal police involve more civilians and therefore, may lead to more civilian casualties if the confrontations are deadly enough (total number of deaths). For the purposes of running a randomForest algorithm, 11 variables containing missing values had to be excluded from the dataset. These variables included law enforcement agencies' lethality rating variables and "newIndex" variables. The variable importance chart also cannot be used to gain definite insight into the causality of the situations, and the explanation above is only a conjecture, since the chart only explains the importance of the variables for predicting the number of civilian deaths, and not the value by which each variable influences the number of civilian deaths.


#### Question 2

##### Part 1

We are creating a binary variable indicating whether any casualties at all occured during a confrontation and then using a conditional logistic model to estimate the effect of involvement of particular law enforcement agencies and military forces on sustaining casualties in a confrontation.

```{r}
table(Confrontations$category)
Confrontations_cat <- Confrontations
Confrontations_cat$anycasualties <- ifelse(Confrontations_cat$category == "no.dead.wounded",
                                        "No", "Yes")
Confrontations_cat$anycasualties <- as.factor(Confrontations_cat$anycasualties)

summary(casualties_present <- glm(anycasualties ~ army + navy + federal.police + municipal.police + afi + ministerial.police +
                             army:navy + navy:municipal.police + army:municipal.police, 
                          family = binomial(link = "logit"), data = Confrontations_cat))

casualties_present$coefficients

# marginal effects
interaction_plot_binary(casualties_present, effect = "army", moderator = "navy", interaction = "army:navy")
```


##### Part 2

conditional hypothesis: look at the effect of the **army** being involved in number of civilians + organized crime wounded when arms are seized (long guns + small arms) and when clips or cartridges are seized

```{r}
conditional_ols <- lm(formula = civ_oc_wounded ~ army + arms_seized + cart_clips_seized + 
                        army * arms_seized + army * cart_clips_seized + vehicles.seized + 
                        navy + afi + federal.police + municipal.police + 
                        ministerial.police + state.police, 
                      data = Confrontations_2)
```

From the output of the model, we can see that the marginal effect of the army on the number wounded when no arms are seized or cartridges/clips seized is -0.416. When arms are seized, the marginal effect is smaller (-0.03) and less significant (p < 0.01). When cartridges or clips are seized, the effect is positive, but is even smaller (0.000085) and less significant (p < 0.05).
